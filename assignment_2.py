# -*- coding: utf-8 -*-
"""Assignment 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pmgLjDTniBM8iB6BVwHruQzXIy4Pl1JO
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom



my_data = pd.read_csv("stat231f23dataset29088864.csv")

nr = my_data.shape[0]

tc = (my_data['city'] == 'chicago').sum()

tc

nf = (my_data['subject.sex'] == "female").sum()

nf

#nfc is number of female in chicago in our data
nfc = ((my_data['subject.sex'] == 'female') & (my_data['city'] == 'chicago')).sum()

nfc

#pc is the value of theta c in the assignment
pc = nfc/tc

#psf is the value of theta sf in the assignment
psf = (nf - nfc)/(nr - tc)

pc

psf

# Defining the parameters for the binomial distribution
n = nfc  # Number of trials
p = 0.36009174311926606 # Probability of success

# Generate a range of values for the number of successes (k)
k_values = np.arange(0, n + 1)

# Calculate the relative likelihood (probability mass function) for each k
likelihood = binom.pmf(k_values, n, p)

# Create a time series plot to visualize the relative likelihood function
plt.plot(k_values, likelihood, linestyle='-')
plt.xlabel('Number of Successes (k)')
plt.ylabel('Relative Likelihood')
plt.title('Relative Likelihood Function for Binomial Distribution for Chicago')
plt.grid(True)
plt.show()

# Defining the parameters for the binomial distribution
n = (nf - nfc) # Number of trials
p = psf # Probability of success

# Generate a range of values for the number of successes (k)
k_values = np.arange(0, n + 1)

# Calculate the relative likelihood (probability mass function) for each k
likelihood = binom.pmf(k_values, n, p)

# Create a time series plot to visualize the relative likelihood function
plt.plot(k_values, likelihood, linestyle='-')
plt.xlabel('Number of Successes (k)')
plt.ylabel('Relative Likelihood')
plt.title('Relative Likelihood Function for Binomial Distribution for SF')
plt.grid(True)
plt.show()

"""Analysis 2
---
"""

sum_of_ages = 0
age_list = my_data["subject.age"].tolist()

mean_age = sum(age_list)/len(age_list)

mean_age

nr

# as nr is odd so there must be a centre most value which will become our median
#which will be : median_age_th
median_age_th = (nr+1)/2
median_age_th

asc_list_age = sorted(age_list)
median_age = asc_list_age[int(median_age_th)]

median_age

#np.std fins the standard deviation of the list
std_deviation = np.std(age_list)

std_deviation

# Create a histogram
plt.figure(figsize=(8, 8))
plt.hist(age_list, bins=20, density=True, alpha=0.6, color='green', label='Relative Frequency Histogram')

# Generate an Exponential PDF curve
x = np.linspace(0, max(age_list), 100)
pdf = (1/np.mean(age_list)) * np.exp((-1/np.mean(age_list)) * x)
plt.plot(x, pdf, 'r-', lw=1, label='Exponential PDF Curve')

# Customize the plot
plt.title('Relative Frequency Histogram with Exponential PDF')
plt.xlabel('Age')
plt.ylabel('Density')
plt.grid(False)
plt.legend()

x = np.linspace(0, 500, 1000)  # This creates 100 evenly spaced points between 0 and 5

# Define the exponential function

y = (1/36.73118279569893)*(np.exp((-x)/36.73118279569893))  # This example uses the exponential function e^x

# Create the plot
plt.figure(figsize=(10, 8))  # Set the figure size (optional)
plt.plot(x, y, label='y = (1/Θ)e^(-x/Θ)', color='blue')
plt.plot()
# Customize the plot (optional)
plt.title('Probability Density Function of subject.age')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.legend()

# Show the plot
plt.show()

import statsmodels.api as sm
import matplotlib.pyplot as plt

# Create a Q-Q plot

sm.qqplot(np.array(age_list), line='s')

# Customize the plot (optional)
plt.title("Q-Q Plot")
plt.show()

"""ANALYSIS 3"""

min(age_list)

import math

e = 2.718281828459045

prob_sum = 0
for i in range(31):
  prob_sum += pow(mean_age,i)/((pow(e,(mean_age)))*math.factorial(i))

prob_sum

(pow(mean_age,39)/((pow(e,(mean_age)))*math.factorial(39)))/(pow(mean_age,37)/((pow(e,(mean_age)))*math.factorial(37)))

mnfc = ["chevrolet","ford","toyota","honda","other"]
num_mnfc = []
for i in mnfc:
  num_veh = ((my_data['vehicle.make'] == i) & (my_data['city'] == 'chicago')).sum()
  num_mnfc.append(num_veh)

num_mnfc

expected_frequency = []
given_l = [0.101,0.128,0.093,0.138,0.54]
for i in given_l:
  expected_frequency.append(round(i*tc,2))
expected_frequency

from prettytable import PrettyTable

new_table = PrettyTable(["Make", "Observed Frequncy", "Expected Frequency"])

for i in range(5):
  new_table.add_row([mnfc[i], num_mnfc[i], expected_frequency[i]])

print(new_table)

# Set the width of the bars and the positions of the bars
bar_width = 0.35
index = range(len(mnfc))

# Create the grouped bar plot
plt.bar(index, num_mnfc, bar_width, label='Observed Frequency')
plt.bar([i + bar_width for i in index], expected_frequency, bar_width, label='Expected Frequency')

# Customize the plot
plt.xlabel('Make')
plt.ylabel('Frequency')
plt.title('Grouped barplot of the observed and expected frequencies')
plt.xticks([i + bar_width/2 for i in index], mnfc)
plt.legend()

# Show the plot
plt.show()

chevy = []
ford = []
toy = []
hond = []
oth = []
for i in my_data['vehicle.make']:
  if i == 'chevrolet':
    chevy.append([subject.age])

my_data.index = range(1, len(my_data) + 1)
my_data

my_data.index

import seaborn as sns

# Sample data
data1 = np.random.normal(0, 32, 100)
data2 = np.random.normal(1, 30, 100)
data3 = np.random.normal(1, 33, 100)
data4 = np.random.normal(2, 36, 100)
data5 = np.random.normal(3, 35, 100)

# Create a DataFrame
import pandas as pd
df = pd.DataFrame({'chevrolet': data1, 'ford': data2, 'toyota': data3, 'honda': data4, 'others': data5})

# Create side-by-side boxplots using seaborn
sns.boxplot(data=df, orient='h')

# Customize the plot
plt.title('Side-by-Side Boxplots')
plt.xlabel('Age')

# Show the plot
plt.show()

max(age_list)

min(age_list)

nr

(my_data['city'] == 'chicago')

latl = my_data["lat"].tolist()

latl = latl[0:436]

latl

lngl = my_data["lng"].tolist()
lngl = lngl[0:436]

lngl

mean_lat = sum(latl) / len(latl)

mean_lng = sum(lngl) / len(lngl)

lat_2_5 = np.percentile(latl, 2.5)
lng_2_5 = np.percentile(lngl, 2.5)

lat_97_5 = np.percentile(latl, 97.5)
lng_97_5 = np.percentile(lngl, 97.5)

sdlat = np.std(latl)
sdlng = np.std(lngl)

mean_lat

mean_lng

lat_2_5

lng_2_5

import numpy as np
from scipy.optimize import minimize_scalar
from scipy.stats import binom

# Define your data
n = 436  # Number of trials
x = 157  # Number of successes
alpha = 0.10  # Significance level (10%)

# Define the likelihood function for a binomial distribution
def likelihood(p):
    return -binom(n, p).pmf(x)  # Negative log-likelihood for minimization

# Find the lower limit of the 10% likelihood interval
result_lower = minimize_scalar(likelihood, bounds=(0, 1), method='bounded')
lower_limit = result_lower.x

# Find the upper limit of the 10% likelihood interval
result_upper = minimize_scalar(likelihood, bounds=(0, 1), method='bounded', options={'xatol': 1e-10})
upper_limit = result_upper.x

# Print the results
print("Number of successes:", x)
print(f"10% Likelihood Interval for p: [{lower_limit}, {upper_limit}]")

from scipy.stats import binom

# Define your data
n = 436  # Number of trials
x = 157  # Number of successes
target_alpha = 0.10  # Target likelihood interval significance level (10%)

# Define a function to calculate the likelihood interval for a given confidence level
def calculate_confidence_interval(confidence_level):
    alpha = 1 - confidence_level

    # Search for a confidence interval that matches the target likelihood interval
    for p in range(1, 101):
        p_value = p / 100
        lower = binom.ppf(alpha / 2, n, p_value)
        upper = binom.ppf(1 - alpha / 2, n, p_value)
        if lower <= x <= upper:
            return p_value, lower, upper

    return None, None, None

# Search for a confidence level that matches the 10% likelihood interval
confidence_level = 0.50  # Start with a 50% confidence interval

while True:
    p_value, lower_limit, upper_limit = calculate_confidence_interval(confidence_level)

    if p_value is not None:
        print(f"Similar confidence level found: {confidence_level}")
        print(f"Confidence Interval for this level (p={p_value:.2f}): [{lower_limit}, {upper_limit}]")
        break

    confidence_level += 0.01

from scipy.stats import uniform

# Define the confidence interval bounds
lower_bound = 41.8393912050567
upper_bound = 41.8531479783517

# Define the specific value
value = 41.8462695917041

# Calculate the probability that the value lies within the interval
probability = uniform.cdf(upper_bound, loc=lower_bound, scale=upper_bound - lower_bound) - uniform.cdf(value, loc=lower_bound, scale=upper_bound - lower_bound)

print(f"The probability that {value} lies in the interval is {probability:.4f}")

lat_97_5

lng_97_5

sdlat

sdlng